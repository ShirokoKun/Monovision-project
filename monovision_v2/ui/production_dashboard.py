"""
MonoVision V3 - Enhanced UI Components
Production-ready interface components with advanced diagnostics and real-time feedback
"""
import gradio as gr
import json
import numpy as np
import asyncio
from typing import Dict, Any, List, Optional, Tuple
from PIL import Image, ImageDraw, ImageFont
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class ProductionDashboard:
    """
    Production-grade dashboard component for MonoVision V3
    Features real-time performance monitoring, system diagnostics, and user guidance
    """
    
    def __init__(self):
        self.performance_history = []
        self.session_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'average_processing_time': 0.0,
            'mode_usage': {'fast': 0, 'balanced': 0, 'rich': 0},
            'session_start': datetime.now()
        }
        
    def create_dashboard_interface(self) -> gr.Blocks:
        """Create comprehensive dashboard interface"""
        
        with gr.Blocks(theme=self._get_custom_theme(), title="MonoVision V3 - Production Dashboard") as dashboard:
            
            # Header with system status
            with gr.Row():
                gr.HTML("""
                <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 10px; margin-bottom: 20px;">
                    <h1 style="color: white; margin: 0; font-size: 2.5em;">üéØ MonoVision V3 Enhanced</h1>
                    <p style="color: #e0e0e0; margin: 10px 0 0 0; font-size: 1.2em;">Production-Ready Multi-Modal AI Platform</p>
                </div>
                """)
            
            # Real-time system metrics
            with gr.Row():
                with gr.Column(scale=1):
                    system_status = gr.JSON(
                        label="üñ•Ô∏è System Status",
                        value=self._get_initial_system_status()
                    )
                
                with gr.Column(scale=1):
                    performance_metrics = gr.Plot(
                        label="üìä Performance Metrics",
                        value=self._create_initial_performance_plot()
                    )
                
                with gr.Column(scale=1):
                    mode_distribution = gr.Plot(
                        label="üéõÔ∏è Mode Usage Distribution", 
                        value=self._create_mode_distribution_plot()
                    )
            
            # Main processing interface
            with gr.Row():
                with gr.Column(scale=2):
                    # Image input with enhanced preview
                    image_input = gr.Image(
                        label="üì∏ Image Input",
                        type="pil",
                        height=400
                    )
                    
                    # Processing controls
                    with gr.Row():
                        mode_selector = gr.Dropdown(
                            choices=["fast", "balanced", "rich"],
                            value="balanced",
                            label="üéõÔ∏è Processing Mode",
                            info="Balanced mode recommended for most tasks"
                        )
                        
                        advanced_options = gr.Checkbox(
                            label="üîß Advanced Options",
                            value=False
                        )
                    
                    # Advanced controls (conditional)
                    with gr.Row(visible=False) as advanced_row:
                        object_detection = gr.Checkbox(
                            label="üéØ Enhanced Object Detection",
                            value=True,
                            info="15 objects max, spatial analysis"
                        )
                        semantic_analysis = gr.Checkbox(
                            label="üß† Advanced Semantic Analysis", 
                            value=True,
                            info="70 keywords across 6 categories"
                        )
                        fusion_layer = gr.Checkbox(
                            label="üîÄ Enhanced Fusion Layer",
                            value=True,
                            info="Context-aware integration"
                        )
                    
                    # Query input with intelligent suggestions
                    query_input = gr.Textbox(
                        label="üí¨ Your Question",
                        placeholder="Ask about objects, mood, composition, or anything you see...",
                        lines=2
                    )
                    
                    # Smart suggestions
                    suggestions_display = gr.HTML(
                        value=self._get_default_suggestions(),
                        label="üí° Smart Suggestions"
                    )
                
                with gr.Column(scale=3):
                    # Enhanced response area
                    with gr.Tabs():
                        with gr.Tab("ü§ñ AI Response"):
                            response_text = gr.Textbox(
                                label="Analysis Result",
                                lines=8,
                                interactive=False
                            )
                            
                            # Response metadata
                            with gr.Row():
                                confidence_score = gr.Number(
                                    label="üéØ Confidence",
                                    precision=2,
                                    interactive=False
                                )
                                processing_time = gr.Number(
                                    label="‚è±Ô∏è Time (s)",
                                    precision=2,
                                    interactive=False
                                )
                                tokens_used = gr.Number(
                                    label="üìù Tokens",
                                    precision=0,
                                    interactive=False
                                )
                        
                        with gr.Tab("üîç Vision Analysis"):
                            vision_results = gr.JSON(
                                label="Detailed Vision Analysis"
                            )
                            
                            object_overlay_image = gr.Image(
                                label="üéØ Object Detection Overlay",
                                interactive=False
                            )
                        
                        with gr.Tab("üß† Reasoning"):
                            reasoning_explanation = gr.Textbox(
                                label="Why this analysis?",
                                lines=6,
                                interactive=False
                            )
                            
                            mode_recommendation = gr.Textbox(
                                label="Mode Selection Reasoning",
                                lines=3,
                                interactive=False
                            )
                        
                        with gr.Tab("üí° Suggestions"):
                            follow_up_suggestions = gr.HTML(
                                label="Intelligent Follow-up Suggestions"
                            )
            
            # Processing button with enhanced feedback
            with gr.Row():
                process_btn = gr.Button(
                    "üöÄ Analyze with MonoVision V3",
                    variant="primary",
                    size="lg"
                )
                
                explain_btn = gr.Button(
                    "‚ùì Explain Response",
                    variant="secondary"
                )
                
                clear_btn = gr.Button(
                    "üóëÔ∏è Clear",
                    variant="stop"
                )
            
            # Progress indicator
            progress_bar = gr.Progress()
            
            # Status messages
            status_message = gr.HTML(
                value="<div style='text-align: center; color: #666;'>Ready to analyze images with enhanced AI capabilities</div>"
            )
            
            # Event handlers
            advanced_options.change(
                fn=lambda x: gr.update(visible=x),
                inputs=[advanced_options],
                outputs=[advanced_row]
            )
            
            # Main processing event
            process_btn.click(
                fn=self._process_with_enhanced_feedback,
                inputs=[
                    image_input, query_input, mode_selector, 
                    object_detection, semantic_analysis, fusion_layer
                ],
                outputs=[
                    response_text, confidence_score, processing_time, tokens_used,
                    vision_results, object_overlay_image, reasoning_explanation,
                    mode_recommendation, follow_up_suggestions, suggestions_display,
                    system_status, performance_metrics, status_message
                ]
            )
            
            # Auto-update system metrics every 30 seconds
            gr.Timer(30).tick(
                fn=self._update_system_metrics,
                outputs=[system_status, performance_metrics, mode_distribution]
            )
        
        return dashboard
    
    def _get_custom_theme(self) -> gr.Theme:
        """Create custom theme for production interface"""
        return gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="purple", 
            neutral_hue="slate",
            font=gr.themes.GoogleFont("Inter")
        ).set(
            body_background_fill="linear-gradient(135deg, #1e3c72 0%, #2a5298 100%)",  # deep blue gradient
            panel_background_fill="rgba(255, 255, 255, 0.85)",                        # soft frosted white
            button_primary_background_fill="linear-gradient(135deg, #ff512f 0%, #dd2476 100%)",  # orange-pink neon
            button_primary_text_color="#ffffff"    
        )
    
    def _get_initial_system_status(self) -> Dict[str, Any]:
        """Get initial system status"""
        return {
            "Status": "üü¢ Online",
            "Version": "V3 Enhanced",
            "GPU": "NVIDIA GTX 1650",
            "VRAM": "4.0GB Available", 
            "Models Loaded": "BLIP + CLIP + Flan-T5",
            "Cache Status": "Active",
            "Uptime": "00:00:00",
            "Total Requests": 0,
            "Success Rate": "100%"
        }
    
    def _create_initial_performance_plot(self) -> go.Figure:
        """Create initial performance monitoring plot"""
        fig = go.Figure()
        
        # Add placeholder data
        fig.add_trace(go.Scatter(
            x=[],
            y=[],
            mode='lines+markers',
            name='Processing Time',
            line=dict(color='#667eea', width=3)
        ))
        
        fig.update_layout(
            title="Processing Time Over Time",
            xaxis_title="Request #",
            yaxis_title="Time (seconds)",
            template="plotly_white",
            height=250,
            margin=dict(l=20, r=20, t=40, b=20)
        )
        
        return fig
    
    def _create_mode_distribution_plot(self) -> go.Figure:
        """Create mode usage distribution plot"""
        fig = go.Figure(data=[
            go.Bar(
                x=['Fast', 'Balanced', 'Rich'],
                y=[0, 0, 0],
                marker_color=['#2ecc71', '#3498db', '#9b59b6'],
                text=[0, 0, 0],
                textposition='auto'
            )
        ])
        
        fig.update_layout(
            title="Processing Mode Usage",
            xaxis_title="Mode",
            yaxis_title="Count",
            template="plotly_white",
            height=250,
            margin=dict(l=20, r=20, t=40, b=20)
        )
        
        return fig
    
    def _get_default_suggestions(self) -> str:
        """Get default smart suggestions HTML"""
        return """
        <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 10px 0;">
            <h4 style="margin-top: 0; color: #495057;">üí° Try asking about:</h4>
            <div style="display: flex; flex-wrap: wrap; gap: 8px;">
                <span style="background: #e3f2fd; padding: 5px 10px; border-radius: 15px; font-size: 12px;">Objects and their positions</span>
                <span style="background: #f3e5f5; padding: 5px 10px; border-radius: 15px; font-size: 12px;">Mood and atmosphere</span>
                <span style="background: #e8f5e8; padding: 5px 10px; border-radius: 15px; font-size: 12px;">Visual style and composition</span>
                <span style="background: #fff3e0; padding: 5px 10px; border-radius: 15px; font-size: 12px;">Spatial relationships</span>
            </div>
        </div>
        """
    
    async def _process_with_enhanced_feedback(
        self,
        image: Optional[Image.Image],
        query: str,
        mode: str,
        enable_objects: bool,
        enable_semantic: bool,
        enable_fusion: bool,
        progress=gr.Progress()
    ) -> Tuple[str, float, float, int, Dict, Optional[Image.Image], str, str, str, str, Dict, go.Figure, str]:
        """
        Process request with enhanced feedback and real-time updates
        """
        
        if not image and not query.strip():
            status_msg = "<div style='color: #e74c3c; text-align: center;'>‚ö†Ô∏è Please provide an image or text query</div>"
            return "", 0.0, 0.0, 0, {}, None, "", "", "", "", {}, go.Figure(), status_msg
        
        try:
            # Update progress
            progress(0.1, desc="Initializing analysis...")
            
            # Simulate processing steps with progress updates
            progress(0.2, desc="Loading models...")
            await asyncio.sleep(0.5)  # Simulate model loading
            
            progress(0.4, desc="Processing image...")
            # Here you would call your actual orchestrator
            # For now, we'll simulate the response
            
            progress(0.7, desc="Generating response...")
            await asyncio.sleep(1.0)  # Simulate generation
            
            progress(0.9, desc="Finalizing results...")
            
            # Simulate enhanced response
            response_text = f"I can see a detailed scene with multiple elements. The image shows {query if query else 'interesting visual content'} with clear composition and good lighting quality."
            confidence = 0.87
            processing_time = 12.5
            tokens_used = 73
            
            # Simulate vision results
            vision_results = {
                "caption": "a detailed scene with multiple objects",
                "objects_detected": 5,
                "semantic_keywords": ["high quality", "well composed", "clear"],
                "fusion_quality": 0.87,
                "spatial_analysis": {
                    "scene_complexity": "medium",
                    "object_distribution": "balanced"
                }
            }
            
            # Create object overlay (simulated)
            overlay_image = self._create_object_overlay(image) if image else None
            
            # Generate reasoning
            reasoning = f"""
**Analysis Approach:**
- Used {mode} mode for optimal balance of detail and processing speed
- Enhanced object detection identified key elements in the scene
- Advanced semantic analysis with 70-keyword vocabulary
- Spatial intelligence analyzed object relationships

**Confidence Factors:**
- High fusion quality ({confidence:.2f})
- Clear object detection results
- Strong semantic keyword matches
- Coherent scene composition
            """
            
            # Mode recommendation explanation
            mode_explanation = f"""
**Why {mode.title()} Mode:**
- Processing time: ~{processing_time:.1f}s (within expected range)
- Token limit: {tokens_used} tokens used efficiently  
- Optimal for your query type and image complexity
- Balances detail with performance for this content
            """
            
            # Generate smart suggestions
            suggestions_html = self._generate_smart_suggestions(vision_results, query)
            
            # Update session stats
            self._update_session_stats(mode, processing_time, True)
            
            # Update system status
            updated_status = self._get_updated_system_status()
            
            # Update performance plot
            updated_performance = self._update_performance_plot(processing_time)
            
            # Success message
            status_msg = f"<div style='color: #27ae60; text-align: center;'>‚úÖ Analysis completed successfully in {processing_time:.1f}s</div>"
            
            progress(1.0, desc="Complete!")
            
            return (
                response_text, confidence, processing_time, tokens_used,
                vision_results, overlay_image, reasoning, mode_explanation,
                suggestions_html, self._get_default_suggestions(),
                updated_status, updated_performance, status_msg
            )
            
        except Exception as e:
            logger.error(f"Error in enhanced processing: {e}")
            self._update_session_stats(mode, 0.0, False)
            error_msg = f"<div style='color: #e74c3c; text-align: center;'>‚ùå Error: {str(e)}</div>"
            return "", 0.0, 0.0, 0, {}, None, "", "", "", "", {}, go.Figure(), error_msg
    
    def _create_object_overlay(self, image: Image.Image) -> Image.Image:
        """Create object detection overlay on image"""
        if not image:
            return None
        
        # Create a copy of the image
        overlay_img = image.copy()
        draw = ImageDraw.Draw(overlay_img)
        
        # Simulate object detection boxes
        width, height = image.size
        
        # Simulated objects with bounding boxes
        simulated_objects = [
            {"name": "person", "bbox": [0.2, 0.1, 0.6, 0.8], "confidence": 0.89},
            {"name": "chair", "bbox": [0.1, 0.6, 0.3, 0.9], "confidence": 0.76},
            {"name": "table", "bbox": [0.5, 0.7, 0.9, 0.95], "confidence": 0.82}
        ]
        
        colors = ["#e74c3c", "#2ecc71", "#3498db", "#f39c12", "#9b59b6"]
        
        try:
            # Try to load a font, fallback to default if not available
            font = ImageFont.truetype("arial.ttf", 16)
        except:
            font = ImageFont.load_default()
        
        for i, obj in enumerate(simulated_objects):
            color = colors[i % len(colors)]
            bbox = obj["bbox"]
            
            # Convert normalized coordinates to pixel coordinates
            x1 = int(bbox[0] * width)
            y1 = int(bbox[1] * height)
            x2 = int(bbox[2] * width)
            y2 = int(bbox[3] * height)
            
            # Draw bounding box
            draw.rectangle([x1, y1, x2, y2], outline=color, width=3)
            
            # Draw label background
            label = f"{obj['name']} ({obj['confidence']:.2f})"
            bbox_text = draw.textbbox((x1, y1), label, font=font)
            draw.rectangle(
                [bbox_text[0]-2, bbox_text[1]-2, bbox_text[2]+2, bbox_text[3]+2],
                fill=color
            )
            
            # Draw label text
            draw.text((x1, y1), label, fill="white", font=font)
        
        return overlay_img
    
    def _generate_smart_suggestions(self, vision_results: Dict, query: str) -> str:
        """Generate intelligent follow-up suggestions"""
        suggestions = []
        
        # Based on vision results
        if vision_results.get("objects_detected", 0) > 3:
            suggestions.append("üéØ Ask about specific objects: 'Tell me about the person in the center'")
        
        if "high quality" in vision_results.get("semantic_keywords", []):
            suggestions.append("üé® Explore style: 'What artistic techniques are used here?'")
        
        if vision_results.get("spatial_analysis", {}).get("scene_complexity") == "medium":
            suggestions.append("üìê Spatial analysis: 'How are the objects arranged in the scene?'")
        
        # Based on query patterns
        if query and any(word in query.lower() for word in ["what", "describe"]):
            suggestions.append("‚ùì Ask 'why' questions: 'Why does this image have that mood?'")
        
        # Default suggestions
        if not suggestions:
            suggestions = [
                "üîç Try: 'What objects are in the foreground vs background?'",
                "üé≠ Ask: 'What mood or atmosphere do you detect?'",
                "üé® Explore: 'Describe the visual style and composition'"
            ]
        
        suggestions_html = f"""
        <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 10px 0;">
            <h4 style="margin-top: 0; color: #495057;">üß† Intelligent Suggestions:</h4>
            <ul style="margin: 10px 0; padding-left: 20px;">
                {"".join(f"<li style='margin: 8px 0; color: #495057;'>{suggestion}</li>" for suggestion in suggestions[:4])}
            </ul>
        </div>
        """
        
        return suggestions_html
    
    def _update_session_stats(self, mode: str, processing_time: float, success: bool):
        """Update session statistics"""
        self.session_stats['total_requests'] += 1
        
        if success:
            self.session_stats['successful_requests'] += 1
            
            # Update average processing time
            current_avg = self.session_stats['average_processing_time']
            total_success = self.session_stats['successful_requests']
            self.session_stats['average_processing_time'] = (
                (current_avg * (total_success - 1) + processing_time) / total_success
            )
            
            # Update mode usage
            if mode in self.session_stats['mode_usage']:
                self.session_stats['mode_usage'][mode] += 1
            
            # Add to performance history
            self.performance_history.append({
                'request_number': self.session_stats['total_requests'],
                'processing_time': processing_time,
                'mode': mode,
                'timestamp': datetime.now()
            })
            
            # Keep only last 50 entries
            if len(self.performance_history) > 50:
                self.performance_history = self.performance_history[-50:]
    
    def _get_updated_system_status(self) -> Dict[str, Any]:
        """Get updated system status with real metrics"""
        success_rate = (
            self.session_stats['successful_requests'] / self.session_stats['total_requests'] * 100
            if self.session_stats['total_requests'] > 0 else 100
        )
        
        uptime = datetime.now() - self.session_stats['session_start']
        uptime_str = str(uptime).split('.')[0]  # Remove microseconds
        
        # Try to get real GPU usage from torch if available
        gpu_info = "NVIDIA GTX 1650"
        vram_info = "4.0GB Total"
        
        try:
            import torch
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                reserved = torch.cuda.memory_reserved(0) / 1024**3
                total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                vram_info = f"{allocated:.1f}GB Used / {total:.1f}GB Total"
        except:
            pass
        
        # Calculate cache efficiency based on recent requests
        cache_efficiency = "Unknown"
        if self.performance_history:
            recent_requests = self.performance_history[-10:]  # Last 10 requests
            if recent_requests:
                avg_time = sum(r['processing_time'] for r in recent_requests) / len(recent_requests)
                if avg_time < 5.0:  # Fast responses likely indicate caching
                    cache_efficiency = "High (85%+)"
                elif avg_time < 15.0:
                    cache_efficiency = "Medium (50-85%)"
                else:
                    cache_efficiency = "Low (<50%)"
        
        return {
            "Status": "üü¢ Online",
            "Version": "V3 Enhanced",
            "GPU": gpu_info, 
            "VRAM": vram_info,
            "Models Loaded": "BLIP + CLIP + Phi-2",
            "Cache Efficiency": cache_efficiency,
            "Uptime": uptime_str,
            "Total Requests": self.session_stats['total_requests'],
            "Success Rate": f"{success_rate:.1f}%",
            "Avg Processing Time": f"{self.session_stats['average_processing_time']:.1f}s"
        }
    
    def _update_performance_plot(self, latest_time: float) -> go.Figure:
        """Update performance monitoring plot"""
        if not self.performance_history:
            return self._create_initial_performance_plot()
        
        fig = go.Figure()
        
        # Extract data
        request_numbers = [entry['request_number'] for entry in self.performance_history]
        processing_times = [entry['processing_time'] for entry in self.performance_history]
        modes = [entry['mode'] for entry in self.performance_history]
        
        # Color-code by mode
        colors = []
        for mode in modes:
            if mode == 'fast':
                colors.append('#2ecc71')
            elif mode == 'balanced':
                colors.append('#3498db')
            else:  # rich
                colors.append('#9b59b6')
        
        fig.add_trace(go.Scatter(
            x=request_numbers,
            y=processing_times,
            mode='lines+markers',
            name='Processing Time',
            line=dict(color='#667eea', width=2),
            marker=dict(color=colors, size=6),
            hovertemplate='Request #%{x}<br>Time: %{y:.1f}s<br>Mode: %{text}<extra></extra>',
            text=modes
        ))
        
        # Add average line
        if len(processing_times) > 1:
            avg_time = sum(processing_times) / len(processing_times)
            fig.add_hline(
                y=avg_time,
                line_dash="dash",
                line_color="red",
                annotation_text=f"Avg: {avg_time:.1f}s"
            )
        
        fig.update_layout(
            title="Processing Time Over Time",
            xaxis_title="Request #",
            yaxis_title="Time (seconds)",
            template="plotly_white",
            height=250,
            margin=dict(l=20, r=20, t=40, b=20),
            showlegend=False
        )
        
        return fig
    
    def _update_system_metrics(self) -> Tuple[Dict, go.Figure, go.Figure]:
        """Update all system metrics (called by timer)"""
        updated_status = self._get_updated_system_status()
        updated_performance = self._update_performance_plot(0.0) if self.performance_history else self._create_initial_performance_plot()
        updated_modes = self._update_mode_distribution_plot()
        
        return updated_status, updated_performance, updated_modes
    
    def get_system_metrics(self) -> Dict[str, Any]:
        """Get comprehensive system metrics for dashboard integration"""
        try:
            # Performance metrics
            performance_metrics = {
                'avg_response_time': self.session_stats['average_processing_time'],
                'success_rate': (
                    self.session_stats['successful_requests'] / self.session_stats['total_requests'] * 100
                    if self.session_stats['total_requests'] > 0 else 100
                ),
                'total_requests': self.session_stats['total_requests'],
                'requests_per_mode': self.session_stats['mode_usage']
            }
            
            # Resource usage
            resource_metrics = {}
            try:
                import psutil
                resource_metrics = {
                    'cpu_percent': psutil.cpu_percent(interval=0.1),
                    'memory_percent': psutil.virtual_memory().percent,
                    'disk_usage_percent': psutil.disk_usage('/').percent if hasattr(psutil, 'disk_usage') else 0
                }
            except ImportError:
                resource_metrics = {'cpu_percent': 0, 'memory_percent': 0, 'disk_usage_percent': 0}
            
            # GPU metrics
            gpu_metrics = {}
            try:
                import torch
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated(0) / 1024**3
                    reserved = torch.cuda.memory_reserved(0) / 1024**3
                    total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    gpu_metrics = {
                        'gpu_memory_allocated_gb': allocated,
                        'gpu_memory_reserved_gb': reserved,
                        'gpu_memory_total_gb': total,
                        'gpu_utilization_percent': (allocated / total) * 100
                    }
            except:
                gpu_metrics = {'gpu_memory_allocated_gb': 0, 'gpu_memory_total_gb': 4.0, 'gpu_utilization_percent': 0}
            
            # Cache analytics
            cache_metrics = {}
            if self.performance_history:
                recent_times = [r['processing_time'] for r in self.performance_history[-20:]]
                if recent_times:
                    avg_recent = sum(recent_times) / len(recent_times)
                    # Estimate cache efficiency based on response times
                    if avg_recent < 3.0:
                        efficiency_score = 9.0  # Very fast = high cache hit
                    elif avg_recent < 8.0:
                        efficiency_score = 7.0  # Medium fast = some caching
                    elif avg_recent < 15.0:
                        efficiency_score = 5.0  # Normal = limited caching
                    else:
                        efficiency_score = 3.0  # Slow = low cache hit
                    
                    cache_metrics = {
                        'efficiency_score': efficiency_score,
                        'avg_response_time_recent': avg_recent,
                        'cache_hit_estimated': efficiency_score >= 7.0
                    }
            
            return {
                'performance': performance_metrics,
                'resource_usage': resource_metrics,
                'gpu_metrics': gpu_metrics,
                'cache_analytics': cache_metrics,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error getting system metrics: {e}")
            return {}

    def create_dashboard_summary_html(self) -> str:
        """Create a concise dashboard summary for integration into main UI"""
        try:
            metrics = self.get_system_metrics()
            
            # Header
            html = """
            <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 15px; border-radius: 8px; margin: 10px 0;">
                <h3 style="color: white; margin-top: 0; text-align: center;">üìä Production Dashboard V3</h3>
            """
            
            # Performance section
            if 'performance' in metrics:
                perf = metrics['performance']
                html += f"""
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px; margin: 10px 0;">
                    <div style="background: rgba(255,255,255,0.1); padding: 8px; border-radius: 4px;">
                        <strong style="color: #e0e0e0;">‚è±Ô∏è Avg Response:</strong><br>
                        <span style="color: white; font-size: 1.2em;">{perf.get('avg_response_time', 0):.1f}s</span>
                    </div>
                    <div style="background: rgba(255,255,255,0.1); padding: 8px; border-radius: 4px;">
                        <strong style="color: #e0e0e0;">‚úÖ Success Rate:</strong><br>
                        <span style="color: white; font-size: 1.2em;">{perf.get('success_rate', 0):.1f}%</span>
                    </div>
                </div>
                """
            
            # System resources
            if 'resource_usage' in metrics and 'gpu_metrics' in metrics:
                resource = metrics['resource_usage']
                gpu = metrics['gpu_metrics']
                html += f"""
                <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 8px; margin: 10px 0;">
                    <div style="background: rgba(255,255,255,0.1); padding: 6px; border-radius: 4px; text-align: center;">
                        <strong style="color: #e0e0e0;">üñ•Ô∏è CPU</strong><br>
                        <span style="color: white;">{resource.get('cpu_percent', 0):.1f}%</span>
                    </div>
                    <div style="background: rgba(255,255,255,0.1); padding: 6px; border-radius: 4px; text-align: center;">
                        <strong style="color: #e0e0e0;">üíæ RAM</strong><br>
                        <span style="color: white;">{resource.get('memory_percent', 0):.1f}%</span>
                    </div>
                    <div style="background: rgba(255,255,255,0.1); padding: 6px; border-radius: 4px; text-align: center;">
                        <strong style="color: #e0e0e0;">üéÆ GPU</strong><br>
                        <span style="color: white;">{gpu.get('gpu_utilization_percent', 0):.1f}%</span>
                    </div>
                </div>
                """
            
            # Cache status
            if 'cache_analytics' in metrics:
                cache = metrics['cache_analytics']
                efficiency = cache.get('efficiency_score', 0)
                cache_status = "üü¢ High" if efficiency >= 7 else "üü° Medium" if efficiency >= 5 else "üî¥ Low"
                html += f"""
                <div style="background: rgba(255,255,255,0.1); padding: 8px; border-radius: 4px; margin: 10px 0;">
                    <strong style="color: #e0e0e0;">üíæ Cache Efficiency:</strong>
                    <span style="color: white; margin-left: 10px;">{cache_status} ({efficiency:.1f}/10)</span>
                </div>
                """
            
            # Total requests
            if 'performance' in metrics:
                total_requests = metrics['performance'].get('total_requests', 0)
                html += f"""
                <div style="text-align: center; margin-top: 10px;">
                    <span style="color: #e0e0e0; font-size: 0.9em;">Total Requests: </span>
                    <span style="color: white; font-weight: bold;">{total_requests}</span>
                </div>
                """
            
            html += "</div>"
            return html
            
        except Exception as e:
            logger.error(f"Error creating dashboard summary: {e}")
            return """
            <div style="background: #e74c3c; padding: 15px; border-radius: 8px; margin: 10px 0;">
                <h3 style="color: white; margin: 0; text-align: center;">üìä Dashboard Error</h3>
                <p style="color: white; margin: 5px 0 0 0; text-align: center;">Unable to load metrics</p>
            </div>
            """

    def _update_mode_distribution_plot(self) -> go.Figure:
        """Update mode usage distribution plot"""
        mode_counts = self.session_stats['mode_usage']
        
        fig = go.Figure(data=[
            go.Bar(
                x=['Fast', 'Balanced', 'Rich'],
                y=[mode_counts['fast'], mode_counts['balanced'], mode_counts['rich']],
                marker_color=['#2ecc71', '#3498db', '#9b59b6'],
                text=[mode_counts['fast'], mode_counts['balanced'], mode_counts['rich']],
                textposition='auto'
            )
        ])
        
        fig.update_layout(
            title="Processing Mode Usage",
            xaxis_title="Mode",
            yaxis_title="Count",
            template="plotly_white",
            height=250,
            margin=dict(l=20, r=20, t=40, b=20)
        )
        
        return fig
        """Update mode usage distribution plot"""
        mode_counts = self.session_stats['mode_usage']
        
        fig = go.Figure(data=[
            go.Bar(
                x=['Fast', 'Balanced', 'Rich'],
                y=[mode_counts['fast'], mode_counts['balanced'], mode_counts['rich']],
                marker_color=['#2ecc71', '#3498db', '#9b59b6'],
                text=[mode_counts['fast'], mode_counts['balanced'], mode_counts['rich']],
                textposition='auto'
            )
        ])
        
        fig.update_layout(
            title="Processing Mode Usage",
            xaxis_title="Mode",
            yaxis_title="Count",
            template="plotly_white",
            height=250,
            margin=dict(l=20, r=20, t=40, b=20)
        )
        
        return fig


class IntelligentModeSelector:
    """
    Intelligent mode selection component with real-time recommendations
    """
    
    def __init__(self):
        self.query_patterns = {
            'fast': [
                r'\b(what is|identify|name|count|how many)\b',
                r'\b(quick|brief|simple|yes|no)\b',
                r'^.{1,20}$'  # Very short queries
            ],
            'balanced': [
                r'\b(describe|explain|analyze|tell me about)\b',
                r'\b(mood|atmosphere|feeling|emotion)\b',
                r'\b(where|position|location|spatial)\b',
                r'\b(objects?|things?|items?)\b'
            ],
            'rich': [
                r'\b(detailed|comprehensive|thorough|in-depth)\b',
                r'\b(artistic|cultural|professional|expert)\b',
                r'\b(composition|lighting|technique|style)\b',
                r'\b(meaning|interpretation|significance)\b'
            ]
        }
    
    def recommend_mode(self, query: str, image_available: bool = False) -> Dict[str, Any]:
        """
        Recommend optimal processing mode based on query analysis
        """
        if not query.strip():
            if image_available:
                return {
                    'recommended_mode': 'balanced',
                    'confidence': 0.8,
                    'reasoning': 'Image available with no specific query - balanced mode provides comprehensive analysis',
                    'alternatives': ['fast', 'rich']
                }
            else:
                return {
                    'recommended_mode': 'fast',
                    'confidence': 0.9,
                    'reasoning': 'No image or query - fast mode for general interaction',
                    'alternatives': ['balanced']
                }
        
        query_lower = query.lower().strip()
        scores = {'fast': 0, 'balanced': 0, 'rich': 0}
        
        # Pattern matching
        import re
        for mode, patterns in self.query_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query_lower, re.IGNORECASE):
                    scores[mode] += 1
        
        # Length-based scoring
        word_count = len(query.split())
        if word_count <= 3:
            scores['fast'] += 2
        elif word_count <= 8:
            scores['balanced'] += 2
        else:
            scores['rich'] += 2
        
        # Complexity indicators
        complexity_indicators = ['why', 'how', 'because', 'detailed', 'comprehensive']
        if any(indicator in query_lower for indicator in complexity_indicators):
            scores['rich'] += 1
            scores['balanced'] += 1
        
        # Spatial indicators
        spatial_indicators = ['where', 'position', 'location', 'left', 'right', 'center', 'behind', 'front']
        if any(indicator in query_lower for indicator in spatial_indicators):
            scores['balanced'] += 2  # Balanced mode excels at spatial analysis
        
        # Find best mode
        recommended_mode = max(scores, key=scores.get)
        max_score = scores[recommended_mode]
        confidence = min(max_score / 5.0, 1.0)  # Normalize to 0-1
        
        # Generate reasoning
        reasoning = self._generate_mode_reasoning(recommended_mode, query, scores)
        
        # Alternative modes
        alternatives = [mode for mode, score in scores.items() 
                       if mode != recommended_mode and score > 0]
        
        return {
            'recommended_mode': recommended_mode,
            'confidence': confidence,
            'reasoning': reasoning,
            'alternatives': alternatives,
            'scores': scores
        }
    
    def _generate_mode_reasoning(self, mode: str, query: str, scores: Dict[str, int]) -> str:
        """Generate human-readable reasoning for mode selection"""
        
        if mode == 'fast':
            return f"""Fast mode recommended because:
‚Ä¢ Query appears to be a simple identification or short question
‚Ä¢ Optimal for quick responses (3-8 seconds)
‚Ä¢ Uses 25 tokens for concise answers
‚Ä¢ Perfect for basic image understanding tasks"""
        
        elif mode == 'balanced':
            return f"""Balanced mode recommended because:
‚Ä¢ Query suggests need for detailed analysis with spatial understanding
‚Ä¢ Enhanced object detection (15 objects, spatial analysis)
‚Ä¢ Advanced semantic analysis (70 keywords across 6 categories)
‚Ä¢ Extended responses (up to 100 tokens) for comprehensive explanations
‚Ä¢ Optimal balance of detail and processing time (~15-30 seconds)"""
        
        else:  # rich
            return f"""Rich mode recommended because:
‚Ä¢ Query indicates need for expert-level analysis
‚Ä¢ Professional-grade insights with cultural context
‚Ä¢ Comprehensive 150-token responses
‚Ä¢ Advanced artistic and technical analysis
‚Ä¢ Best for complex interpretations (~30-60 seconds)"""
    
    def create_mode_selector_interface(self) -> gr.Column:
        """Create intelligent mode selector interface"""
        
        with gr.Column() as selector:
            # Mode selection with real-time recommendation
            mode_selector = gr.Dropdown(
                choices=["fast", "balanced", "rich"],
                value="balanced",
                label="üéõÔ∏è Processing Mode",
                info="AI will recommend optimal mode based on your query"
            )
            
            # Real-time recommendation display
            recommendation_display = gr.HTML(
                value=self._get_default_recommendation_html(),
                label="üß† AI Recommendation"
            )
            
            # Mode comparison info
            mode_info = gr.HTML(
                value=self._get_mode_comparison_html(),
                label="üìä Mode Comparison"
            )
            
            # Performance prediction
            performance_prediction = gr.HTML(
                value="<div style='text-align: center; color: #666;'>Enter a query to see performance prediction</div>",
                label="‚è±Ô∏è Performance Prediction"
            )
        
        return selector, mode_selector, recommendation_display, performance_prediction
    
    def _get_default_recommendation_html(self) -> str:
        """Get default recommendation HTML"""
        return """
        <div style="background: #e3f2fd; padding: 15px; border-radius: 8px; border-left: 4px solid #2196f3;">
            <h4 style="margin-top: 0; color: #1976d2;">üß† AI Recommendation</h4>
            <p style="margin: 0; color: #424242;">Balanced mode is recommended as the default choice for most image analysis tasks, providing the best combination of detail and efficiency.</p>
        </div>
        """
    
    def _get_mode_comparison_html(self) -> str:
        """Get mode comparison HTML"""
        return """
        <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; margin-top: 10px;">
            <h4 style="margin-top: 0; color: #495057;">üìä Mode Comparison</h4>
            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin-top: 10px;">
                <div style="background: #d4edda; padding: 10px; border-radius: 5px; text-align: center;">
                    <strong>üöÄ Fast</strong><br>
                    <small>25 tokens<br>3-8 seconds<br>Quick answers</small>
                </div>
                <div style="background: #cce5ff; padding: 10px; border-radius: 5px; text-align: center;">
                    <strong>‚öñÔ∏è Balanced</strong><br>
                    <small>100 tokens<br>15-30 seconds<br>Detailed analysis</small>
                </div>
                <div style="background: #e2c2ff; padding: 10px; border-radius: 5px; text-align: center;">
                    <strong>üî¨ Rich</strong><br>
                    <small>150 tokens<br>30-60 seconds<br>Expert insights</small>
                </div>
            </div>
        </div>
        """
    
    def update_recommendation(self, query: str, image_available: bool = True) -> Tuple[str, str]:
        """Update recommendation based on query"""
        recommendation = self.recommend_mode(query, image_available)
        
        # Create recommendation HTML
        recommended_mode = recommendation['recommended_mode']
        confidence = recommendation['confidence']
        reasoning = recommendation['reasoning']
        
        recommendation_html = f"""
        <div style="background: #e3f2fd; padding: 15px; border-radius: 8px; border-left: 4px solid #2196f3;">
            <h4 style="margin-top: 0; color: #1976d2;">üß† AI Recommendation: {recommended_mode.title()} Mode</h4>
            <div style="margin: 10px 0;">
                <div style="background: #fff; padding: 8px; border-radius: 4px; margin: 5px 0;">
                    <strong>Confidence:</strong> {confidence:.0%}
                </div>
                <div style="background: #fff; padding: 8px; border-radius: 4px; margin: 5px 0;">
                    <strong>Reasoning:</strong><br>{reasoning}
                </div>
            </div>
        </div>
        """
        
        # Create performance prediction
        performance_html = self._generate_performance_prediction(recommended_mode, query)
        
        return recommendation_html, performance_html
    
    def _generate_performance_prediction(self, mode: str, query: str) -> str:
        """Generate performance prediction HTML"""
        
        predictions = {
            'fast': {
                'time': '3-8 seconds',
                'tokens': '‚â§25 tokens',
                'vram': '~1.5GB',
                'features': ['Quick identification', 'Basic analysis']
            },
            'balanced': {
                'time': '15-30 seconds', 
                'tokens': '‚â§100 tokens',
                'vram': '~3.5GB',
                'features': ['Enhanced object detection', 'Spatial analysis', 'Advanced semantics']
            },
            'rich': {
                'time': '30-60 seconds',
                'tokens': '‚â§150 tokens', 
                'vram': '~2GB + 8GB RAM',
                'features': ['Expert analysis', 'Cultural context', 'Professional insights']
            }
        }
        
        pred = predictions[mode]
        
        return f"""
        <div style="background: #f1f8e9; padding: 15px; border-radius: 8px; border-left: 4px solid #4caf50;">
            <h4 style="margin-top: 0; color: #2e7d32;">‚è±Ô∏è Performance Prediction ({mode.title()} Mode)</h4>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
                <div style="background: #fff; padding: 8px; border-radius: 4px;">
                    <strong>‚è∞ Time:</strong> {pred['time']}
                </div>
                <div style="background: #fff; padding: 8px; border-radius: 4px;">
                    <strong>üìù Tokens:</strong> {pred['tokens']}
                </div>
                <div style="background: #fff; padding: 8px; border-radius: 4px;">
                    <strong>üíæ VRAM:</strong> {pred['vram']}
                </div>
                <div style="background: #fff; padding: 8px; border-radius: 4px;">
                    <strong>üéØ Features:</strong> {len(pred['features'])} enhanced
                </div>
            </div>
            <div style="margin-top: 10px; background: #fff; padding: 8px; border-radius: 4px;">
                <strong>‚ú® Capabilities:</strong> {', '.join(pred['features'])}
            </div>
        </div>
        """
